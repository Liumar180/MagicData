package connectTomysql

import org.apache.spark.rdd.JdbcRDD
import org.apache.spark.SparkConf
import org.apache.spark.SparkContext
import java.util.ArrayList
import java.sql.DriverManager
import org.apache.spark.rdd.RDD.rddToPairRDDFunctions


object SparkToJDBC {
  def main(args: Array[String]): Unit = {
    val url = "jdbc:mysql://192.168.40.59:3306/story"
    val username = "root"
    val password = "123456"

    /*Driver program*/
    //构建spark 应用程序的运行环境SparkConf
    val conf = new SparkConf().setAppName("ReadDataTest").setMaster("local[3]")
    
    //初始化Sparkcontext
    val sc = new SparkContext(conf)  
    sc.addJar("lib/mysql-connector-java-5.1.10-bin.jar");
    Class.forName("com.mysql.jdbc.Driver").newInstance()
    
    //Executor
    val myRDD = new JdbcRDD(sc, () => DriverManager.getConnection(url, username, password),
      "select sip,dip,account from covert_channel limit ?, ?",
      0, 30020, 1, r => r.getString("sip") + "," + r.getString("dip") + "," + r.getString("account"))

    
    //myRDD.foreach(println(_))
    var arr = new ArrayList[Any];
    
    //Transformation，数据集中的每个元素，经过map中的function转换成另一个新的RDD 
    val myRdd2 = myRDD.map { s =>
      val parts = s.split(",")
      (parts(0).trim(), parts(1).trim())
    }

    //Transformation，将key进行分组，再用function处理Value，返回新的RDD
    val myRdd3 = myRdd2.reduceByKey((x, y) =>
      (x + " " + y)).collect
    //Action， collect将以数组的形式返回整个数据集的所有内容给驱动程序，key值相同的累加

    myRdd3.map(s => {
      val parts1 = s._2.split(" ")
      if (parts1.size >= 2) {
        arr.add(parts1(0).trim())   // + "\t" + parts1(1).trim() + "\t" + parts1(2).trim()
      }
      arr
    })
    
    //并行化集合，使用Parallelize()方法转换成RDD
    val list2 = arr.toArray();
    val result = sc.parallelize(list2, 1);
    
    //写入RDD到文件
    result.saveAsTextFile("data/jdbcrdd3");
    
    //关闭Sparkcontext
    sc.stop();
  }
  
 
}